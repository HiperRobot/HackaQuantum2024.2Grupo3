{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.2-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.0-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.54.1-cp312-cp312-win_amd64.whl.metadata (167 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.7-cp312-cp312-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\leo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\leo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (24.1)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.0.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\leo\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\leo\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.9.2-cp312-cp312-win_amd64.whl (7.8 MB)\n",
      "   ---------------------------------------- 0.0/7.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/7.8 MB 2.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.0/7.8 MB 2.5 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.6/7.8 MB 2.5 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.4/7.8 MB 2.6 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 2.9/7.8 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 3.7/7.8 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.2/7.8 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.7/7.8 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 6.0/7.8 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.8/7.8 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.8/7.8 MB 3.3 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.0-cp312-cp312-win_amd64.whl (218 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.54.1-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 5.0 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.7-cp312-cp312-win_amd64.whl (55 kB)\n",
      "Downloading pillow-11.0.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------- ----------- 1.8/2.6 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.6/2.6 MB 4.8 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.54.1 kiwisolver-1.4.7 matplotlib-3.9.2 pillow-11.0.0 pyparsing-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts fonttools.exe, pyftmerge.exe, pyftsubset.exe and ttx.exe are installed in 'c:\\Users\\Leo\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primeiras linhas do dataframe\n",
      "Index(['Legal Fund Name Including Umbrella', 'Fund Code', 'Citi Code', 'ISIN',\n",
      "       'WKN', 'Apir Code', 'SEDOL', 'As Of Date', 'ESG Score', 'ESG Rating',\n",
      "       ...\n",
      "       'BIC Of Transfer Agent', 'Domicile Of Transfer Agent',\n",
      "       'Marketmaker Name', 'Dissemination Recipient',\n",
      "       'Global Intermediary Identification Number Of Fund',\n",
      "       'Sub-Investment Advisor Name', 'Auditor Name', 'Fund Promoter Name',\n",
      "       'EmailAddressOfManCoIdentifier', 'IsUNPRISignatoryIdentifier'],\n",
      "      dtype='object', length=2206)\n"
     ]
    }
   ],
   "source": [
    "print(\"primeiras linhas do dataframe\")\n",
    "\n",
    "# Definindo o tamanho do chunk para x linhas\n",
    "chunksize = 200\n",
    "reader = pd.read_csv('Quali_20240616.csv', chunksize=chunksize)\n",
    "\n",
    "# Pegando o primeiro chunk\n",
    "chunk = next(reader)\n",
    "\n",
    "# Exibindo o chunk\n",
    "print(chunk.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primeiras linhas do dataframe\n",
      "         Legal Fund Name Including Umbrella Fund Code Citi Code          ISIN  \\\n",
      "0                                      C 11     00F9A      009A  AT0000721402   \n",
      "1                                      C 11     00F9A      009A  AT0000721402   \n",
      "2                                  R-VIP 50     00F9G      009G  AT0000A0FA24   \n",
      "3                                  R-VIP 50     00F9G      009G  AT0000A0FA24   \n",
      "4                                 R-VIP 100     00F9I      009I  AT0000A0F9X2   \n",
      "5                                 R-VIP 100     00F9I      009I  AT0000A0F9X2   \n",
      "6                          Raiffeisen R 135     00F9K      009K  AT0000691365   \n",
      "7                          Raiffeisen R 135     00F9K      009K  AT0000691365   \n",
      "8  JPMorgan Funds - Europe Equity Plus Fund     AEFD9      00A2  LU0289214545   \n",
      "9  JPMorgan Funds - Europe Equity Plus Fund     AEFD9      00A2  LU0289214545   \n",
      "\n",
      "      WKN  Apir Code    SEDOL  As Of Date  ESG Score  ESG Rating  ...  \\\n",
      "0  A0J4QW        NaN      NaN         NaN        NaN         NaN  ...   \n",
      "1  A0J4QW        NaN      NaN         NaN        NaN         NaN  ...   \n",
      "2  A0YBNA        NaN      NaN         NaN        NaN         NaN  ...   \n",
      "3  A0YBNA        NaN      NaN         NaN        NaN         NaN  ...   \n",
      "4  A0YBM7        NaN      NaN         NaN        NaN         NaN  ...   \n",
      "5  A0YBM7        NaN      NaN         NaN        NaN         NaN  ...   \n",
      "6  A0MTVZ        NaN      NaN         NaN        NaN         NaN  ...   \n",
      "7  A0MTVZ        NaN      NaN         NaN        NaN         NaN  ...   \n",
      "8  A0MNZ6        NaN  B1VWZB7         NaN        NaN         NaN  ...   \n",
      "9  A0MNZ6        NaN  B1VWZB7         NaN        NaN         NaN  ...   \n",
      "\n",
      "   BIC Of Transfer Agent  Domicile Of Transfer Agent  Marketmaker Name  \\\n",
      "0                    NaN                         NaN               NaN   \n",
      "1                    NaN                         NaN               NaN   \n",
      "2                    NaN                         NaN               NaN   \n",
      "3                    NaN                         NaN               NaN   \n",
      "4                    NaN                         NaN               NaN   \n",
      "5                    NaN                         NaN               NaN   \n",
      "6                    NaN                         NaN               NaN   \n",
      "7                    NaN                         NaN               NaN   \n",
      "8                    NaN                         NaN               NaN   \n",
      "9                    NaN                         NaN               NaN   \n",
      "\n",
      "   Dissemination Recipient  Global Intermediary Identification Number Of Fund  \\\n",
      "0                      NaN                                BI33Z7.99999.SL.040   \n",
      "1                      NaN                                BI33Z7.99999.SL.040   \n",
      "2                      NaN                                I5ACQV.99999.SL.040   \n",
      "3                      NaN                                I5ACQV.99999.SL.040   \n",
      "4                      NaN                                U5QQTP.99999.SL.040   \n",
      "5                      NaN                                U5QQTP.99999.SL.040   \n",
      "6                      NaN                                FKKMZQ.99999.SL.040   \n",
      "7                      NaN                                FKKMZQ.99999.SL.040   \n",
      "8                      NaN                                5SRHAW.99999.SL.442   \n",
      "9                      NaN                                5SRHAW.99999.SL.442   \n",
      "\n",
      "   Sub-Investment Advisor Name                                 Auditor Name  \\\n",
      "0                          NaN                            KPMG Austria GmbH   \n",
      "1                          NaN                            KPMG Austria GmbH   \n",
      "2                          NaN                            KPMG Austria GmbH   \n",
      "3                          NaN                            KPMG Austria GmbH   \n",
      "4                          NaN                            KPMG Austria GmbH   \n",
      "5                          NaN                            KPMG Austria GmbH   \n",
      "6                          NaN                            KPMG Austria GmbH   \n",
      "7                          NaN                            KPMG Austria GmbH   \n",
      "8                          NaN  PriceWaterhouse Coopers Société Coopérative   \n",
      "9                          NaN  PriceWaterhouse Coopers Société Coopérative   \n",
      "\n",
      "            Fund Promoter Name  EmailAddressOfManCoIdentifier  \\\n",
      "0                     Kathrein                            NaN   \n",
      "1                     Kathrein                            NaN   \n",
      "2                          NaN                            NaN   \n",
      "3                          NaN                            NaN   \n",
      "4                          NaN                            NaN   \n",
      "5                          NaN                            NaN   \n",
      "6                          NaN                            NaN   \n",
      "7                          NaN                            NaN   \n",
      "8  JPMorgan Bank Luxembourg SA                  @jpmorgan.com   \n",
      "9  JPMorgan Bank Luxembourg SA                  @jpmorgan.com   \n",
      "\n",
      "   IsUNPRISignatoryIdentifier  \n",
      "0                         NaN  \n",
      "1                         NaN  \n",
      "2                         NaN  \n",
      "3                         NaN  \n",
      "4                         NaN  \n",
      "5                         NaN  \n",
      "6                         NaN  \n",
      "7                         NaN  \n",
      "8                         Yes  \n",
      "9                         Yes  \n",
      "\n",
      "[10 rows x 2206 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"primeiras linhas do dataframe\")\n",
    "\n",
    "# Definindo o tamanho do chunk para x linhas\n",
    "chunksize = 200\n",
    "reader = pd.read_csv('Quali_20240617.csv', chunksize=chunksize)\n",
    "\n",
    "# Pegando o primeiro chunk\n",
    "chunk = next(reader)\n",
    "\n",
    "# Exibindo o chunk\n",
    "print(chunk.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Legal Fund Name Including Umbrella', 'Fund Code', 'Citi Code', 'ISIN', 'WKN', '20000_Financial_Instrument_Identifying_Data', 'EPT Portfolio Name', 'EPT Portfolio or Share Class Currency', 'EPT PRIIPs KID Publication Date', 'EPT Reference Language', 'EPT Valuation Frequency', 'EPT Summary Risk Indicator', 'EPT Is SRI Adjusted', 'EPT Market Risk Measure', 'EPT Credit Risk Measure', 'EPT Recommended Holding Period', 'EPT Number Of Observed Returns', 'EPT Reference Invested Amount', 'EPT Ongoing Costs Management Fees and Other Administrative or Operating Costs', 'EPT Portfolio Transaction Costs', 'EPT Target Market Retail Investor Type', 'EPT Investment Objective', 'EPT Risk Narrative', 'EPT Other Risk Narrative', 'EPT Investment Option', 'Portfolio Turnover Ratio', 'Portfolio Turnover Ratio Date', 'FE Group Name', 'Unit Status', 'Company Contact Address', 'Company Contact E-Mail', 'Company Contact Post Code', 'Company Contact Telephone', 'Company Contact Web Address', 'FE Fund Name', 'Sub Category', 'Main Category', 'Global Fund Classification', 'Has Performance Fee', 'Performance Fee Applied', 'Custodian Fee Applied', 'Subscription Fee Maximum', 'Redemption Fee Maximum', 'Management Fee Applied', 'Management Fee Applied Reference Date', 'Management Fee Maximum', 'TER Excluding Performance Fee', 'TER Excluding Performance Fee Date', 'TER Including Performance Fee', 'TER Including Performance Fee Date', 'Has Ongoing Charges', 'Ongoing Charges', 'Ongoing Charges Date', 'Distribution Fee', 'Has Separate Distribution Fee', 'Distribution Fee Reference Date', 'Minimal Subscription Category', 'Minimal Initial Subscription In Shares', 'Minimal Initial Subscription In Amount', 'Currency Of Minimal Subscription', 'Minimal Subsequent Subscription Category', 'Minimal Subsequent Subscription In Shares', 'Minimal Subsequent Subscription In Amount', 'Maximal Number Of Possible Decimals NAV', 'Settlement Period For Subscription', 'EFAMA Main EFC Category', 'EFAMA Active EFC Classification', 'Is EU Directive Relevant', 'Type Of EU Directive', 'UCITS Version', 'Legal Form', 'Launch Price', 'Launch Price Currency', 'Launch Price Date', 'Listing Currency', 'Valor', 'Share Class Extension', 'Full Share Class Name', 'Abbreviated Share Class Name', 'Valuation Frequency', 'Share Class Distribution Policy', 'Share Class Currency', 'Share Class Lifecycle', 'Share Class Launch Date', 'Investment Status', 'Benchmark', 'SRRI', 'Record Date For SRRI', 'CurrencyHedgeShareClassIdentifier', 'DistributionDeclarationFrequencyIdentifier', 'Asset Class', 'Sector Names', 'Fund Domicile Alpha-2', 'Legal Fund Name Only', 'Fund Launch Date', 'Investment Objective in English', 'Fund Currency', 'Open-ended Or Closed-ended Fund Structure', 'CurrencyHedgePortfolioIdentifier', 'Domicile', 'Has Umbrella', 'Umbrella', 'Domicile Of Umbrella', 'Fund Group Name', 'ManCo', 'Domicile Of ManCo', 'Fund Administrator Name', 'Custodian Bank Name', 'Portfolio Managing Company Name', 'Transfer Agent Name', 'Auditor Name', 'Fund Promoter Name', 'Exit Charge', 'Is ETF']\n"
     ]
    }
   ],
   "source": [
    "with open(\"colunas_especificas.txt\", 'r') as fin:\n",
    "    cols = fin.readlines()\n",
    "    cols_especificas = []\n",
    "    for c in cols:\n",
    "        cols_especificas.append(c.replace(\"\\n\",\"\"))\n",
    "\n",
    "\n",
    "print(cols_especificas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo as primeiras 100 linhas do arquivo Quali_20240601.csv\n",
      "As primeiras 100 linhas de Quali_20240601.csv foram processadas e armazenadas como chunk_01\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240602.csv\n",
      "As primeiras 100 linhas de Quali_20240602.csv foram processadas e armazenadas como chunk_02\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240603.csv\n",
      "As primeiras 100 linhas de Quali_20240603.csv foram processadas e armazenadas como chunk_03\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240604.csv\n",
      "As primeiras 100 linhas de Quali_20240604.csv foram processadas e armazenadas como chunk_04\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240605.csv\n",
      "As primeiras 100 linhas de Quali_20240605.csv foram processadas e armazenadas como chunk_05\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240606.csv\n",
      "As primeiras 100 linhas de Quali_20240606.csv foram processadas e armazenadas como chunk_06\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240607.csv\n",
      "As primeiras 100 linhas de Quali_20240607.csv foram processadas e armazenadas como chunk_07\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240608.csv\n",
      "As primeiras 100 linhas de Quali_20240608.csv foram processadas e armazenadas como chunk_08\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240609.csv\n",
      "As primeiras 100 linhas de Quali_20240609.csv foram processadas e armazenadas como chunk_09\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240610.csv\n",
      "As primeiras 100 linhas de Quali_20240610.csv foram processadas e armazenadas como chunk_10\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240611.csv\n",
      "As primeiras 100 linhas de Quali_20240611.csv foram processadas e armazenadas como chunk_11\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240612.csv\n",
      "As primeiras 100 linhas de Quali_20240612.csv foram processadas e armazenadas como chunk_12\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240613.csv\n",
      "As primeiras 100 linhas de Quali_20240613.csv foram processadas e armazenadas como chunk_13\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240614.csv\n",
      "As primeiras 100 linhas de Quali_20240614.csv foram processadas e armazenadas como chunk_14\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240615.csv\n",
      "As primeiras 100 linhas de Quali_20240615.csv foram processadas e armazenadas como chunk_15\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240616.csv\n",
      "As primeiras 100 linhas de Quali_20240616.csv foram processadas e armazenadas como chunk_16\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240617.csv\n",
      "As primeiras 100 linhas de Quali_20240617.csv foram processadas e armazenadas como chunk_17\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240618.csv\n",
      "As primeiras 100 linhas de Quali_20240618.csv foram processadas e armazenadas como chunk_18\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240619.csv\n",
      "As primeiras 100 linhas de Quali_20240619.csv foram processadas e armazenadas como chunk_19\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240620.csv\n",
      "As primeiras 100 linhas de Quali_20240620.csv foram processadas e armazenadas como chunk_20\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240621.csv\n",
      "As primeiras 100 linhas de Quali_20240621.csv foram processadas e armazenadas como chunk_21\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240622.csv\n",
      "As primeiras 100 linhas de Quali_20240622.csv foram processadas e armazenadas como chunk_22\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240623.csv\n",
      "As primeiras 100 linhas de Quali_20240623.csv foram processadas e armazenadas como chunk_23\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240624.csv\n",
      "As primeiras 100 linhas de Quali_20240624.csv foram processadas e armazenadas como chunk_24\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240625.csv\n",
      "As primeiras 100 linhas de Quali_20240625.csv foram processadas e armazenadas como chunk_25\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240626.csv\n",
      "As primeiras 100 linhas de Quali_20240626.csv foram processadas e armazenadas como chunk_26\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240627.csv\n",
      "As primeiras 100 linhas de Quali_20240627.csv foram processadas e armazenadas como chunk_27\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240628.csv\n",
      "As primeiras 100 linhas de Quali_20240628.csv foram processadas e armazenadas como chunk_28\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240629.csv\n",
      "As primeiras 100 linhas de Quali_20240629.csv foram processadas e armazenadas como chunk_29\n",
      "Lendo as primeiras 100 linhas do arquivo Quali_20240630.csv\n",
      "As primeiras 100 linhas de Quali_20240630.csv foram processadas e armazenadas como chunk_30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Defina o tamanho do chunk para ler as primeiras N linhas\n",
    "chunksize = 100  # Lê apenas as primeiras 100 linhas\n",
    "\n",
    "# Dicionário para armazenar os DataFrames processados\n",
    "chunks_dict = {}\n",
    "\n",
    "for xx in range(1, 31):\n",
    "    # Formatar o número com dois dígitos\n",
    "    xx_formatted = f\"{xx:02d}\"\n",
    "    filename = f\"Quali_202406{xx_formatted}.csv\"\n",
    "    \n",
    "    print(f\"Lendo as primeiras {chunksize} linhas do arquivo {filename}\")\n",
    "    \n",
    "    try:\n",
    "        # Criar um leitor de chunks para o arquivo atual\n",
    "        reader = pd.read_csv(filename, chunksize=chunksize)\n",
    "        \n",
    "        # Obter o primeiro chunk\n",
    "        chunk = next(reader)\n",
    "        \n",
    "        # Verificar quais colunas estão presentes no chunk\n",
    "        cols_to_keep = [col for col in cols_especificas if col in chunk.columns]\n",
    "        \n",
    "        # Se nenhuma coluna for encontrada, pular o arquivo\n",
    "        if not cols_to_keep:\n",
    "            print(f\"Nenhuma das colunas especificadas foi encontrada em {filename}\")\n",
    "            continue\n",
    "        \n",
    "        # Filtrar as colunas específicas\n",
    "        chunk_filtered = chunk[cols_to_keep]\n",
    "        \n",
    "        # Armazenar o chunk filtrado no dicionário\n",
    "        chunks_dict[f\"chunk_{xx_formatted}\"] = chunk_filtered\n",
    "        \n",
    "        print(f\"As primeiras {chunksize} linhas de {filename} foram processadas e armazenadas como chunk_{xx_formatted}\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"O arquivo {filename} não foi encontrado. Pulando para o próximo arquivo.\")\n",
    "        continue\n",
    "\n",
    "# Agora você pode acessar cada DataFrame processado a partir do dicionário 'chunks_dict'\n",
    "# Por exemplo:\n",
    "# df_chunk_01 = chunks_dict['chunk_01']\n",
    "# print(df_chunk_01.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Legal Fund Name Including Umbrella Fund Code Citi Code          ISIN  \\\n",
      "0                               C 11     00F9A      009A  AT0000721402   \n",
      "1                               C 11     00F9A      009A  AT0000721402   \n",
      "2                           R-VIP 50     00F9G      009G  AT0000A0FA24   \n",
      "3                           R-VIP 50     00F9G      009G  AT0000A0FA24   \n",
      "4                          R-VIP 100     00F9I      009I  AT0000A0F9X2   \n",
      "\n",
      "      WKN 20000_Financial_Instrument_Identifying_Data EPT Portfolio Name  \\\n",
      "0  A0J4QW                                AT0000721402             C 11 A   \n",
      "1  A0J4QW                                         NaN             C 11 A   \n",
      "2  A0YBNA                                AT0000A0FA24         R-VIP 50 T   \n",
      "3  A0YBNA                                         NaN         R-VIP 50 T   \n",
      "4  A0YBM7                                AT0000A0F9X2    R-VIP 100 (R) T   \n",
      "\n",
      "  EPT Portfolio or Share Class Currency EPT PRIIPs KID Publication Date  \\\n",
      "0                                   EUR                      2024-04-30   \n",
      "1                                   EUR                      2024-04-30   \n",
      "2                                   EUR                      2024-04-30   \n",
      "3                                   EUR                      2024-04-30   \n",
      "4                                   EUR                      2024-04-30   \n",
      "\n",
      "  EPT Reference Language  ...                                           ManCo  \\\n",
      "0                    deu  ...  Raiffeisen Kapitalanlage-Gesellschaft m. b. H.   \n",
      "1                    deu  ...  Raiffeisen Kapitalanlage-Gesellschaft m. b. H.   \n",
      "2                    deu  ...  Raiffeisen Kapitalanlage-Gesellschaft m. b. H.   \n",
      "3                    deu  ...  Raiffeisen Kapitalanlage-Gesellschaft m. b. H.   \n",
      "4                    deu  ...  Raiffeisen Kapitalanlage-Gesellschaft m. b. H.   \n",
      "\n",
      "   Domicile Of ManCo Fund Administrator Name  \\\n",
      "0                 AT                     NaN   \n",
      "1                 AT                     NaN   \n",
      "2                 AT                     NaN   \n",
      "3                 AT                     NaN   \n",
      "4                 AT                     NaN   \n",
      "\n",
      "                Custodian Bank Name  \\\n",
      "0  Raiffeisen Bank International AG   \n",
      "1  Raiffeisen Bank International AG   \n",
      "2  Raiffeisen Bank International AG   \n",
      "3  Raiffeisen Bank International AG   \n",
      "4  Raiffeisen Bank International AG   \n",
      "\n",
      "                Portfolio Managing Company Name  \\\n",
      "0        Kathrein Privatbank Aktiengesellschaft   \n",
      "1        Kathrein Privatbank Aktiengesellschaft   \n",
      "2  Raiffeisen Kapitalanlage-Gesellschaft m.b.H.   \n",
      "3  Raiffeisen Kapitalanlage-Gesellschaft m.b.H.   \n",
      "4  Raiffeisen Kapitalanlage-Gesellschaft m.b.H.   \n",
      "\n",
      "                        Transfer Agent Name       Auditor Name  \\\n",
      "0  Raiffeisen Bank International AG, Vienna  KPMG Austria GmbH   \n",
      "1  Raiffeisen Bank International AG, Vienna  KPMG Austria GmbH   \n",
      "2  Raiffeisen Bank International AG, Vienna  KPMG Austria GmbH   \n",
      "3  Raiffeisen Bank International AG, Vienna  KPMG Austria GmbH   \n",
      "4  Raiffeisen Bank International AG, Vienna  KPMG Austria GmbH   \n",
      "\n",
      "   Fund Promoter Name  Exit Charge  Is ETF  \n",
      "0            Kathrein          NaN      No  \n",
      "1            Kathrein          NaN      No  \n",
      "2                 NaN          NaN      No  \n",
      "3                 NaN          NaN      No  \n",
      "4                 NaN          NaN      No  \n",
      "\n",
      "[5 rows x 114 columns]\n"
     ]
    }
   ],
   "source": [
    "df_chunk_01 = chunks_dict['chunk_01']\n",
    "print(df_chunk_01.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              ISIN Citi Code EPT Reference Language  \\\n",
      "0     AT0000691365      009K                    deu   \n",
      "1     AT0000691365      009K                    deu   \n",
      "2     AT0000691365      009K                    deu   \n",
      "3     AT0000691365      009K                    deu   \n",
      "4     AT0000691365      009K                    deu   \n",
      "...            ...       ...                    ...   \n",
      "1058  LU0638090042      00FZ                    swe   \n",
      "1059  LU0638090042      00FZ                    swe   \n",
      "1060  LU0638090042      00FZ                    swe   \n",
      "1061  LU0638090042      00FZ                    swe   \n",
      "1062  LU0638090042      00FZ                    swe   \n",
      "\n",
      "                                            campo  \\\n",
      "0     20000_Financial_Instrument_Identifying_Data   \n",
      "1     20000_Financial_Instrument_Identifying_Data   \n",
      "2     20000_Financial_Instrument_Identifying_Data   \n",
      "3     20000_Financial_Instrument_Identifying_Data   \n",
      "4     20000_Financial_Instrument_Identifying_Data   \n",
      "...                                           ...   \n",
      "1058              EPT Portfolio Transaction Costs   \n",
      "1059                          Custodian Bank Name   \n",
      "1060              EPT PRIIPs KID Publication Date   \n",
      "1061              EPT Portfolio Transaction Costs   \n",
      "1062                          Custodian Bank Name   \n",
      "\n",
      "                           valor_antigo                         valor_novo  \\\n",
      "0                          AT0000691365                                NaN   \n",
      "1                                   NaN                       AT0000691365   \n",
      "2                          AT0000691365                                NaN   \n",
      "3                                   NaN                       AT0000691365   \n",
      "4                          AT0000691365                                NaN   \n",
      "...                                 ...                                ...   \n",
      "1058                           0.002708                           0.002542   \n",
      "1059  J.P. Morgan SE, Luxembourg Branch  J.P. MORGAN SE, LUXEMBOURG BRANCH   \n",
      "1060                         2024-04-30                         2024-05-31   \n",
      "1061                           0.002542                           0.002589   \n",
      "1062  J.P. MORGAN SE, LUXEMBOURG BRANCH  J.P. Morgan SE, Luxembourg Branch   \n",
      "\n",
      "      data  \n",
      "0        1  \n",
      "1        2  \n",
      "2        2  \n",
      "3        3  \n",
      "4        3  \n",
      "...    ...  \n",
      "1058     3  \n",
      "1059    25  \n",
      "1060    27  \n",
      "1061    27  \n",
      "1062    29  \n",
      "\n",
      "[1063 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Supondo que você já tem o dicionário 'chunks_dict' com os DataFrames para cada dia\n",
    "# E que as colunas identificadoras são 'ISIN', 'Citi Code', 'EPT Reference Language'\n",
    "\n",
    "# Lista para armazenar os DataFrames com a coluna 'data' adicionada\n",
    "dfs_with_date = []\n",
    "\n",
    "for xx in range(1, 31):\n",
    "    xx_formatted = f\"{xx:02d}\"\n",
    "    key = f\"chunk_{xx_formatted}\"\n",
    "    if key in chunks_dict:\n",
    "        df = chunks_dict[key].copy()\n",
    "        df['data'] = int(xx)  # Adiciona a coluna 'data' com o valor do dia\n",
    "        dfs_with_date.append(df)\n",
    "\n",
    "# Combina todos os DataFrames em um único DataFrame\n",
    "combined_df = pd.concat(dfs_with_date, ignore_index=True)\n",
    "\n",
    "# Ordena o DataFrame pelos identificadores e pela data\n",
    "combined_df.sort_values(by=['ISIN', 'Citi Code', 'EPT Reference Language', 'data'], inplace=True)\n",
    "\n",
    "# Lista para armazenar as mudanças detectadas\n",
    "changes = []\n",
    "\n",
    "# Definir as colunas identificadoras e as colunas a serem comparadas\n",
    "id_cols = ['ISIN', 'Citi Code', 'EPT Reference Language']\n",
    "value_cols = [col for col in combined_df.columns if col not in id_cols + ['data']]\n",
    "\n",
    "# Agrupa o DataFrame pelos identificadores\n",
    "grouped = combined_df.groupby(id_cols)\n",
    "\n",
    "for name, group in grouped:\n",
    "    group = group.sort_values(by='data')\n",
    "    group = group.reset_index(drop=True)\n",
    "    for i in range(1, len(group)):\n",
    "        current_row = group.loc[i]\n",
    "        previous_row = group.loc[i-1]\n",
    "        data_current = current_row['data']\n",
    "        data_previous = previous_row['data']\n",
    "        for col in value_cols:\n",
    "            value_current = current_row[col]\n",
    "            value_previous = previous_row[col]\n",
    "            # Verifica se os valores são diferentes, considerando valores nulos\n",
    "            if pd.isnull(value_current) and pd.isnull(value_previous):\n",
    "                continue  # Ambos são nulos, não há mudança\n",
    "            elif value_current != value_previous:\n",
    "                changes.append({\n",
    "                    'ISIN': current_row['ISIN'],\n",
    "                    'Citi Code': current_row['Citi Code'],\n",
    "                    'EPT Reference Language': current_row['EPT Reference Language'],\n",
    "                    'campo': col,\n",
    "                    'valor_antigo': value_previous,\n",
    "                    'valor_novo': value_current,\n",
    "                    'data': data_current\n",
    "                })\n",
    "\n",
    "# Cria um DataFrame com as mudanças\n",
    "changes_df = pd.DataFrame(changes)\n",
    "\n",
    "# Exibe o DataFrame com as mudanças\n",
    "print(changes_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            campo  \\\n",
      "0     20000_Financial_Instrument_Identifying_Data   \n",
      "1     20000_Financial_Instrument_Identifying_Data   \n",
      "2     20000_Financial_Instrument_Identifying_Data   \n",
      "3     20000_Financial_Instrument_Identifying_Data   \n",
      "4     20000_Financial_Instrument_Identifying_Data   \n",
      "...                                           ...   \n",
      "1058              EPT Portfolio Transaction Costs   \n",
      "1059                          Custodian Bank Name   \n",
      "1060              EPT PRIIPs KID Publication Date   \n",
      "1061              EPT Portfolio Transaction Costs   \n",
      "1062                          Custodian Bank Name   \n",
      "\n",
      "                           valor_antigo                         valor_novo  \\\n",
      "0                          AT0000691365                                NaN   \n",
      "1                                   NaN                       AT0000691365   \n",
      "2                          AT0000691365                                NaN   \n",
      "3                                   NaN                       AT0000691365   \n",
      "4                          AT0000691365                                NaN   \n",
      "...                                 ...                                ...   \n",
      "1058                           0.002708                           0.002542   \n",
      "1059  J.P. Morgan SE, Luxembourg Branch  J.P. MORGAN SE, LUXEMBOURG BRANCH   \n",
      "1060                         2024-04-30                         2024-05-31   \n",
      "1061                           0.002542                           0.002589   \n",
      "1062  J.P. MORGAN SE, LUXEMBOURG BRANCH  J.P. Morgan SE, Luxembourg Branch   \n",
      "\n",
      "      data  \n",
      "0        1  \n",
      "1        2  \n",
      "2        2  \n",
      "3        3  \n",
      "4        3  \n",
      "...    ...  \n",
      "1058     3  \n",
      "1059    25  \n",
      "1060    27  \n",
      "1061    27  \n",
      "1062    29  \n",
      "\n",
      "[1063 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Seleciona apenas as colunas desejadas e imprime o DataFrame\n",
    "print(changes_df[['campo', 'valor_antigo', 'valor_novo', 'data']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            campo  \\\n",
      "0     20000_Financial_Instrument_Identifying_Data   \n",
      "1     20000_Financial_Instrument_Identifying_Data   \n",
      "2     20000_Financial_Instrument_Identifying_Data   \n",
      "3     20000_Financial_Instrument_Identifying_Data   \n",
      "4     20000_Financial_Instrument_Identifying_Data   \n",
      "...                                           ...   \n",
      "1058              EPT Portfolio Transaction Costs   \n",
      "1059                          Custodian Bank Name   \n",
      "1060              EPT PRIIPs KID Publication Date   \n",
      "1061              EPT Portfolio Transaction Costs   \n",
      "1062                          Custodian Bank Name   \n",
      "\n",
      "                           valor_antigo                         valor_novo  \\\n",
      "0                          AT0000691365                                NaN   \n",
      "1                                   NaN                       AT0000691365   \n",
      "2                          AT0000691365                                NaN   \n",
      "3                                   NaN                       AT0000691365   \n",
      "4                          AT0000691365                                NaN   \n",
      "...                                 ...                                ...   \n",
      "1058                           0.002708                           0.002542   \n",
      "1059  J.P. Morgan SE, Luxembourg Branch  J.P. MORGAN SE, LUXEMBOURG BRANCH   \n",
      "1060                         2024-04-30                         2024-05-31   \n",
      "1061                           0.002542                           0.002589   \n",
      "1062  J.P. MORGAN SE, LUXEMBOURG BRANCH  J.P. Morgan SE, Luxembourg Branch   \n",
      "\n",
      "            data  \n",
      "0     01/06/2024  \n",
      "1     02/06/2024  \n",
      "2     02/06/2024  \n",
      "3     03/06/2024  \n",
      "4     03/06/2024  \n",
      "...          ...  \n",
      "1058  03/06/2024  \n",
      "1059  25/06/2024  \n",
      "1060  27/06/2024  \n",
      "1061  27/06/2024  \n",
      "1062  29/06/2024  \n",
      "\n",
      "[1063 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Supondo que você já tem o dicionário 'chunks_dict' com os DataFrames para cada dia\n",
    "# E que as colunas identificadoras são 'ISIN', 'Citi Code', 'EPT Reference Language'\n",
    "\n",
    "# Lista para armazenar os DataFrames com a coluna 'data' adicionada\n",
    "dfs_with_date = []\n",
    "\n",
    "for xx in range(1, 31):\n",
    "    xx_formatted = f\"{xx:02d}\"\n",
    "    key = f\"chunk_{xx_formatted}\"\n",
    "    if key in chunks_dict:\n",
    "        df = chunks_dict[key].copy()\n",
    "        # Adiciona a coluna 'data' com o valor do dia no formato 'xx/06/2024'\n",
    "        df['data'] = f\"{xx_formatted}/06/2024\"\n",
    "        dfs_with_date.append(df)\n",
    "\n",
    "# Combina todos os DataFrames em um único DataFrame\n",
    "combined_df = pd.concat(dfs_with_date, ignore_index=True)\n",
    "\n",
    "# Converter a coluna 'data' para datetime para facilitar a ordenação\n",
    "combined_df['data'] = pd.to_datetime(combined_df['data'], format='%d/%m/%Y')\n",
    "\n",
    "# Ordena o DataFrame pelos identificadores e pela data\n",
    "combined_df.sort_values(by=['ISIN', 'Citi Code', 'EPT Reference Language', 'data'], inplace=True)\n",
    "\n",
    "# Lista para armazenar as mudanças detectadas\n",
    "changes = []\n",
    "\n",
    "# Definir as colunas identificadoras e as colunas a serem comparadas\n",
    "id_cols = ['ISIN', 'Citi Code', 'EPT Reference Language']\n",
    "value_cols = [col for col in combined_df.columns if col not in id_cols + ['data']]\n",
    "\n",
    "# Agrupa o DataFrame pelos identificadores\n",
    "grouped = combined_df.groupby(id_cols)\n",
    "\n",
    "for name, group in grouped:\n",
    "    group = group.sort_values(by='data')\n",
    "    group = group.reset_index(drop=True)\n",
    "    for i in range(1, len(group)):\n",
    "        current_row = group.loc[i]\n",
    "        previous_row = group.loc[i-1]\n",
    "        data_current = current_row['data'].strftime('%d/%m/%Y')  # Converte para string no formato desejado\n",
    "        data_previous = previous_row['data'].strftime('%d/%m/%Y')\n",
    "        for col in value_cols:\n",
    "            value_current = current_row[col]\n",
    "            value_previous = previous_row[col]\n",
    "            # Verifica se os valores são diferentes, considerando valores nulos\n",
    "            if pd.isnull(value_current) and pd.isnull(value_previous):\n",
    "                continue  # Ambos são nulos, não há mudança\n",
    "            elif pd.isnull(value_current) != pd.isnull(value_previous):\n",
    "                # Um valor é nulo e o outro não\n",
    "                changes.append({\n",
    "                    'campo': col,\n",
    "                    'valor_antigo': value_previous,\n",
    "                    'valor_novo': value_current,\n",
    "                    'data': data_current\n",
    "                })\n",
    "            elif value_current != value_previous:\n",
    "                changes.append({\n",
    "                    'campo': col,\n",
    "                    'valor_antigo': value_previous,\n",
    "                    'valor_novo': value_current,\n",
    "                    'data': data_current\n",
    "                })\n",
    "\n",
    "# Cria um DataFrame com as mudanças\n",
    "changes_df = pd.DataFrame(changes)\n",
    "\n",
    "# Seleciona apenas as colunas desejadas\n",
    "changes_df = changes_df[['campo', 'valor_antigo', 'valor_novo', 'data']]\n",
    "\n",
    "# Exibe o DataFrame com as mudanças\n",
    "print(changes_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           campo  valor_antigo    valor_novo  \\\n",
      "0    20000_Financial_Instrument_Identifying_Data  AT0000691365           NaN   \n",
      "149  20000_Financial_Instrument_Identifying_Data  AT0000721402           NaN   \n",
      "411  20000_Financial_Instrument_Identifying_Data  AT0000A0FA24           NaN   \n",
      "321  20000_Financial_Instrument_Identifying_Data  AT0000A0F9X2           NaN   \n",
      "523  20000_Financial_Instrument_Identifying_Data  IE00B61TKZ25           NaN   \n",
      "..                                           ...           ...           ...   \n",
      "54   20000_Financial_Instrument_Identifying_Data           NaN  AT0000691365   \n",
      "203  20000_Financial_Instrument_Identifying_Data           NaN  AT0000721402   \n",
      "584  20000_Financial_Instrument_Identifying_Data  IE00B61TKZ25           NaN   \n",
      "261  20000_Financial_Instrument_Identifying_Data  AT0000779392           NaN   \n",
      "583  20000_Financial_Instrument_Identifying_Data           NaN  IE00B61TKZ25   \n",
      "\n",
      "           data  \n",
      "0    01/06/2024  \n",
      "149  01/06/2024  \n",
      "411  01/06/2024  \n",
      "321  01/06/2024  \n",
      "523  01/06/2024  \n",
      "..          ...  \n",
      "54   30/06/2024  \n",
      "203  30/06/2024  \n",
      "584  30/06/2024  \n",
      "261  30/06/2024  \n",
      "583  30/06/2024  \n",
      "\n",
      "[1063 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Supondo que você já tem o dicionário 'chunks_dict' com os DataFrames para cada dia\n",
    "# E que as colunas identificadoras são 'ISIN', 'Citi Code', 'EPT Reference Language'\n",
    "\n",
    "# Lista para armazenar os DataFrames com a coluna 'data' adicionada\n",
    "dfs_with_date = []\n",
    "\n",
    "for xx in range(1, 31):\n",
    "    xx_formatted = f\"{xx:02d}\"\n",
    "    key = f\"chunk_{xx_formatted}\"\n",
    "    if key in chunks_dict:\n",
    "        df = chunks_dict[key].copy()\n",
    "        # Adiciona a coluna 'data' com o valor do dia no formato 'xx/06/2024'\n",
    "        df['data'] = f\"{xx_formatted}/06/2024\"\n",
    "        dfs_with_date.append(df)\n",
    "\n",
    "# Combina todos os DataFrames em um único DataFrame\n",
    "combined_df = pd.concat(dfs_with_date, ignore_index=True)\n",
    "\n",
    "# Converter a coluna 'data' para datetime para facilitar a ordenação\n",
    "combined_df['data'] = pd.to_datetime(combined_df['data'], format='%d/%m/%Y')\n",
    "\n",
    "# Ordena o DataFrame pelos identificadores e pela data\n",
    "combined_df.sort_values(by=['ISIN', 'Citi Code', 'EPT Reference Language', 'data'], inplace=True)\n",
    "\n",
    "# Lista para armazenar as mudanças detectadas\n",
    "changes = []\n",
    "\n",
    "# Definir as colunas identificadoras e as colunas a serem comparadas\n",
    "id_cols = ['ISIN', 'Citi Code', 'EPT Reference Language']\n",
    "value_cols = [col for col in combined_df.columns if col not in id_cols + ['data']]\n",
    "\n",
    "# Agrupa o DataFrame pelos identificadores\n",
    "grouped = combined_df.groupby(id_cols)\n",
    "\n",
    "for name, group in grouped:\n",
    "    group = group.sort_values(by='data')\n",
    "    group = group.reset_index(drop=True)\n",
    "    for i in range(1, len(group)):\n",
    "        current_row = group.loc[i]\n",
    "        previous_row = group.loc[i-1]\n",
    "        data_current = current_row['data'].strftime('%d/%m/%Y')  # Converte para string no formato desejado\n",
    "        data_previous = previous_row['data'].strftime('%d/%m/%Y')\n",
    "        for col in value_cols:\n",
    "            value_current = current_row[col]\n",
    "            value_previous = previous_row[col]\n",
    "            # Verifica se os valores são diferentes, considerando valores nulos\n",
    "            if pd.isnull(value_current) and pd.isnull(value_previous):\n",
    "                continue  # Ambos são nulos, não há mudança\n",
    "            elif pd.isnull(value_current) != pd.isnull(value_previous):\n",
    "                # Um valor é nulo e o outro não\n",
    "                changes.append({\n",
    "                    'campo': col,\n",
    "                    'valor_antigo': value_previous,\n",
    "                    'valor_novo': value_current,\n",
    "                    'data': data_current\n",
    "                })\n",
    "            elif value_current != value_previous:\n",
    "                changes.append({\n",
    "                    'campo': col,\n",
    "                    'valor_antigo': value_previous,\n",
    "                    'valor_novo': value_current,\n",
    "                    'data': data_current\n",
    "                })\n",
    "\n",
    "# Cria um DataFrame com as mudanças\n",
    "changes_df = pd.DataFrame(changes)\n",
    "\n",
    "# Seleciona apenas as colunas desejadas\n",
    "changes_df = changes_df[['campo', 'valor_antigo', 'valor_novo', 'data']]\n",
    "\n",
    "# Converter a coluna 'data' para datetime para ordenação\n",
    "changes_df['data'] = pd.to_datetime(changes_df['data'], format='%d/%m/%Y')\n",
    "\n",
    "# Ordena o DataFrame 'changes_df' pela coluna 'data'\n",
    "changes_df.sort_values(by='data', inplace=True)\n",
    "\n",
    "# Opcional: Converte a coluna 'data' de volta para string no formato desejado\n",
    "changes_df['data'] = changes_df['data'].dt.strftime('%d/%m/%Y')\n",
    "\n",
    "# Exibe o DataFrame com as mudanças ordenadas por data\n",
    "print(changes_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_01_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_02_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_03_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_04_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_05_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_06_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_07_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_08_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_09_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_10_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_11_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_12_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_13_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_14_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_15_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_16_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_17_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_18_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_19_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_20_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_21_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_22_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_23_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_24_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_25_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_26_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_27_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_28_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_29_sorted.csv' lido com sucesso.\n",
      "Arquivo 'prio_30_sorted.csv' lido com sucesso.\n",
      "              ISIN Citi Code EPT Reference Language  \\\n",
      "0     AT0000607254      03X1                    deu   \n",
      "201   AT0000636964      03X5                    deu   \n",
      "5179  LU0607983383      015B                    deu   \n",
      "291   AT0000642780      03WW                    deu   \n",
      "5118  LU0605964849      03Z1                    eng   \n",
      "...            ...       ...                    ...   \n",
      "585   AT0000711593      041L                    deu   \n",
      "584   AT0000711593      041L                    deu   \n",
      "960   AT0000859236      03SR                    deu   \n",
      "2172  IE00B61TKZ25      00AL                    dan   \n",
      "4698  LU0562087980      03ZG                    eng   \n",
      "\n",
      "                                            campo  valor_antigo    valor_novo  \\\n",
      "0     20000_Financial_Instrument_Identifying_Data  AT0000607254           NaN   \n",
      "201   20000_Financial_Instrument_Identifying_Data  AT0000636964           NaN   \n",
      "5179  20000_Financial_Instrument_Identifying_Data  LU0607983383           NaN   \n",
      "291   20000_Financial_Instrument_Identifying_Data  AT0000642780           NaN   \n",
      "5118  20000_Financial_Instrument_Identifying_Data  LU0605964849           NaN   \n",
      "...                                           ...           ...           ...   \n",
      "585   20000_Financial_Instrument_Identifying_Data  AT0000711593           NaN   \n",
      "584   20000_Financial_Instrument_Identifying_Data           NaN  AT0000711593   \n",
      "960   20000_Financial_Instrument_Identifying_Data  AT0000859236           NaN   \n",
      "2172  20000_Financial_Instrument_Identifying_Data           NaN  IE00B61TKZ25   \n",
      "4698  20000_Financial_Instrument_Identifying_Data           NaN  LU0562087980   \n",
      "\n",
      "            data  \n",
      "0     01/06/2024  \n",
      "201   01/06/2024  \n",
      "5179  01/06/2024  \n",
      "291   01/06/2024  \n",
      "5118  01/06/2024  \n",
      "...          ...  \n",
      "585   30/06/2024  \n",
      "584   30/06/2024  \n",
      "960   30/06/2024  \n",
      "2172  30/06/2024  \n",
      "4698  30/06/2024  \n",
      "\n",
      "[5911 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Defina as colunas identificadoras\n",
    "id_cols = ['ISIN', 'Citi Code', 'EPT Reference Language']\n",
    "\n",
    "# Lista para armazenar os DataFrames com a coluna 'data' adicionada\n",
    "dfs_with_date = []\n",
    "\n",
    "for xx in range(1, 31):\n",
    "    xx_formatted = f\"{xx:02d}\"\n",
    "    filename = f\"prio_{xx_formatted}_sorted.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Ler o arquivo 'prio_xx_sorted.csv'\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "        # Adiciona a coluna 'data' com o valor do dia no formato 'xx/06/2024'\n",
    "        df['data'] = f\"{xx_formatted}/06/2024\"\n",
    "        \n",
    "        # Certifique-se de que as colunas identificadoras estão presentes\n",
    "        missing_cols = [col for col in id_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"As seguintes colunas estão ausentes no arquivo {filename}: {missing_cols}\")\n",
    "            continue\n",
    "        \n",
    "        dfs_with_date.append(df)\n",
    "        print(f\"Arquivo '{filename}' lido com sucesso.\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"O arquivo '{filename}' não foi encontrado. Pulando para o próximo arquivo.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro ao processar o arquivo '{filename}': {e}\")\n",
    "        continue\n",
    "\n",
    "# Combina todos os DataFrames em um único DataFrame\n",
    "combined_df = pd.concat(dfs_with_date, ignore_index=True)\n",
    "\n",
    "# Converter a coluna 'data' para datetime para facilitar a ordenação\n",
    "combined_df['data'] = pd.to_datetime(combined_df['data'], format='%d/%m/%Y')\n",
    "\n",
    "# Ordena o DataFrame pelos identificadores e pela data\n",
    "combined_df.sort_values(by=id_cols + ['data'], inplace=True)\n",
    "\n",
    "# Lista para armazenar as mudanças detectadas\n",
    "changes = []\n",
    "\n",
    "# Definir as colunas a serem comparadas (todas exceto as identificadoras e 'data')\n",
    "value_cols = [col for col in combined_df.columns if col not in id_cols + ['data']]\n",
    "\n",
    "# Agrupa o DataFrame pelos identificadores\n",
    "grouped = combined_df.groupby(id_cols)\n",
    "\n",
    "for name, group in grouped:\n",
    "    group = group.sort_values(by='data').reset_index(drop=True)\n",
    "    for i in range(1, len(group)):\n",
    "        current_row = group.loc[i]\n",
    "        previous_row = group.loc[i - 1]\n",
    "        data_current = current_row['data'].strftime('%d/%m/%Y')  # Converte para string no formato desejado\n",
    "        data_previous = previous_row['data'].strftime('%d/%m/%Y')\n",
    "        for col in value_cols:\n",
    "            value_current = current_row[col]\n",
    "            value_previous = previous_row[col]\n",
    "            # Verifica se os valores são diferentes, considerando valores nulos\n",
    "            if pd.isnull(value_current) and pd.isnull(value_previous):\n",
    "                continue  # Ambos são nulos, não há mudança\n",
    "            elif pd.isnull(value_current) != pd.isnull(value_previous):\n",
    "                # Um valor é nulo e o outro não\n",
    "                changes.append({\n",
    "                    'ISIN': current_row['ISIN'],\n",
    "                    'Citi Code': current_row['Citi Code'],\n",
    "                    'EPT Reference Language': current_row['EPT Reference Language'],\n",
    "                    'campo': col,\n",
    "                    'valor_antigo': value_previous,\n",
    "                    'valor_novo': value_current,\n",
    "                    'data': data_current\n",
    "                })\n",
    "            elif value_current != value_previous:\n",
    "                changes.append({\n",
    "                    'ISIN': current_row['ISIN'],\n",
    "                    'Citi Code': current_row['Citi Code'],\n",
    "                    'EPT Reference Language': current_row['EPT Reference Language'],\n",
    "                    'campo': col,\n",
    "                    'valor_antigo': value_previous,\n",
    "                    'valor_novo': value_current,\n",
    "                    'data': data_current\n",
    "                })\n",
    "\n",
    "# Cria um DataFrame com as mudanças\n",
    "changes_df = pd.DataFrame(changes)\n",
    "\n",
    "# Converter a coluna 'data' para datetime para ordenação\n",
    "changes_df['data'] = pd.to_datetime(changes_df['data'], format='%d/%m/%Y')\n",
    "\n",
    "# Ordena o DataFrame 'changes_df' pela coluna 'data'\n",
    "changes_df.sort_values(by='data', inplace=True)\n",
    "\n",
    "# Opcional: Converte a coluna 'data' de volta para string no formato desejado\n",
    "changes_df['data'] = changes_df['data'].dt.strftime('%d/%m/%Y')\n",
    "\n",
    "# Seleciona apenas as colunas desejadas\n",
    "changes_df = changes_df[['ISIN', 'Citi Code', 'EPT Reference Language', 'campo', 'valor_antigo', 'valor_novo', 'data']]\n",
    "\n",
    "# Exibe o DataFrame com as mudanças ordenadas por data\n",
    "print(changes_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISIN</th>\n",
       "      <th>Citi Code</th>\n",
       "      <th>EPT Reference Language</th>\n",
       "      <th>campo</th>\n",
       "      <th>valor_antigo</th>\n",
       "      <th>valor_novo</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AT0000607254</td>\n",
       "      <td>03X1</td>\n",
       "      <td>deu</td>\n",
       "      <td>20000_Financial_Instrument_Identifying_Data</td>\n",
       "      <td>AT0000607254</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01/06/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>AT0000636964</td>\n",
       "      <td>03X5</td>\n",
       "      <td>deu</td>\n",
       "      <td>20000_Financial_Instrument_Identifying_Data</td>\n",
       "      <td>AT0000636964</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01/06/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5179</th>\n",
       "      <td>LU0607983383</td>\n",
       "      <td>015B</td>\n",
       "      <td>deu</td>\n",
       "      <td>20000_Financial_Instrument_Identifying_Data</td>\n",
       "      <td>LU0607983383</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01/06/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>AT0000642780</td>\n",
       "      <td>03WW</td>\n",
       "      <td>deu</td>\n",
       "      <td>20000_Financial_Instrument_Identifying_Data</td>\n",
       "      <td>AT0000642780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01/06/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5118</th>\n",
       "      <td>LU0605964849</td>\n",
       "      <td>03Z1</td>\n",
       "      <td>eng</td>\n",
       "      <td>20000_Financial_Instrument_Identifying_Data</td>\n",
       "      <td>LU0605964849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01/06/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>AT0000711593</td>\n",
       "      <td>041L</td>\n",
       "      <td>deu</td>\n",
       "      <td>20000_Financial_Instrument_Identifying_Data</td>\n",
       "      <td>AT0000711593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30/06/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>AT0000711593</td>\n",
       "      <td>041L</td>\n",
       "      <td>deu</td>\n",
       "      <td>20000_Financial_Instrument_Identifying_Data</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AT0000711593</td>\n",
       "      <td>30/06/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>AT0000859236</td>\n",
       "      <td>03SR</td>\n",
       "      <td>deu</td>\n",
       "      <td>20000_Financial_Instrument_Identifying_Data</td>\n",
       "      <td>AT0000859236</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30/06/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2172</th>\n",
       "      <td>IE00B61TKZ25</td>\n",
       "      <td>00AL</td>\n",
       "      <td>dan</td>\n",
       "      <td>20000_Financial_Instrument_Identifying_Data</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IE00B61TKZ25</td>\n",
       "      <td>30/06/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4698</th>\n",
       "      <td>LU0562087980</td>\n",
       "      <td>03ZG</td>\n",
       "      <td>eng</td>\n",
       "      <td>20000_Financial_Instrument_Identifying_Data</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LU0562087980</td>\n",
       "      <td>30/06/2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5911 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ISIN Citi Code EPT Reference Language  \\\n",
       "0     AT0000607254      03X1                    deu   \n",
       "201   AT0000636964      03X5                    deu   \n",
       "5179  LU0607983383      015B                    deu   \n",
       "291   AT0000642780      03WW                    deu   \n",
       "5118  LU0605964849      03Z1                    eng   \n",
       "...            ...       ...                    ...   \n",
       "585   AT0000711593      041L                    deu   \n",
       "584   AT0000711593      041L                    deu   \n",
       "960   AT0000859236      03SR                    deu   \n",
       "2172  IE00B61TKZ25      00AL                    dan   \n",
       "4698  LU0562087980      03ZG                    eng   \n",
       "\n",
       "                                            campo  valor_antigo    valor_novo  \\\n",
       "0     20000_Financial_Instrument_Identifying_Data  AT0000607254           NaN   \n",
       "201   20000_Financial_Instrument_Identifying_Data  AT0000636964           NaN   \n",
       "5179  20000_Financial_Instrument_Identifying_Data  LU0607983383           NaN   \n",
       "291   20000_Financial_Instrument_Identifying_Data  AT0000642780           NaN   \n",
       "5118  20000_Financial_Instrument_Identifying_Data  LU0605964849           NaN   \n",
       "...                                           ...           ...           ...   \n",
       "585   20000_Financial_Instrument_Identifying_Data  AT0000711593           NaN   \n",
       "584   20000_Financial_Instrument_Identifying_Data           NaN  AT0000711593   \n",
       "960   20000_Financial_Instrument_Identifying_Data  AT0000859236           NaN   \n",
       "2172  20000_Financial_Instrument_Identifying_Data           NaN  IE00B61TKZ25   \n",
       "4698  20000_Financial_Instrument_Identifying_Data           NaN  LU0562087980   \n",
       "\n",
       "            data  \n",
       "0     01/06/2024  \n",
       "201   01/06/2024  \n",
       "5179  01/06/2024  \n",
       "291   01/06/2024  \n",
       "5118  01/06/2024  \n",
       "...          ...  \n",
       "585   30/06/2024  \n",
       "584   30/06/2024  \n",
       "960   30/06/2024  \n",
       "2172  30/06/2024  \n",
       "4698  30/06/2024  \n",
       "\n",
       "[5911 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_11088\\620246353.py:16: DtypeWarning: Columns (101,739,1150,1151,1178,1179,1180,1181,1183,1931,1968,1971,1984,1990,2042,2043,2061,2076,2110,2126,2175,2183) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_11088\\620246353.py:16: DtypeWarning: Columns (4,101,739,1150,1151,1178,1179,1180,1181,1183,1931,1968,1984,2042,2043,2061,2076,2110,2126,2175,2183) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_11088\\620246353.py:16: DtypeWarning: Columns (101,739,1150,1151,1178,1179,1180,1181,1183,1968,1984,1989,2061,2175) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_01.csv' salvo com sucesso.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Nome do arquivo de entrada\n",
    "input_filename = 'Quali_20240601.csv'\n",
    "\n",
    "# Nome do arquivo de saída\n",
    "output_filename = 'prio_01.csv'\n",
    "\n",
    "# Defina o tamanho do chunk (ajuste conforme necessário)\n",
    "chunksize = 100000  # Ajuste conforme o tamanho do seu arquivo e memória disponível\n",
    "\n",
    "# Criar um leitor de chunks para o arquivo de entrada, lendo apenas as colunas necessárias\n",
    "reader = pd.read_csv(input_filename, usecols=cols_especificas, chunksize=chunksize)\n",
    "\n",
    "# Inicializa um flag para controle de escrita do cabeçalho\n",
    "write_header = True\n",
    "\n",
    "for chunk in reader:\n",
    "    # Escrever o chunk diretamente no arquivo de saída\n",
    "    chunk.to_csv(output_filename, mode='w' if write_header else 'a', index=False, header=write_header)\n",
    "    \n",
    "    # Após a primeira escrita, não precisamos mais escrever o cabeçalho\n",
    "    write_header = False\n",
    "\n",
    "print(f\"Arquivo '{output_filename}' salvo com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\506643535.py:7: DtypeWarning: Columns (27) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('prio_01.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_01_sorted.csv' salvo com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Definir as colunas para ordenar\n",
    "id_cols = ['ISIN', 'Citi Code', 'EPT Reference Language']\n",
    "\n",
    "# Ler o arquivo 'prio_01.csv'\n",
    "df = pd.read_csv('prio_01.csv')\n",
    "\n",
    "# Verificar se as colunas para ordenação estão presentes\n",
    "missing_cols = [col for col in id_cols if col not in df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"As seguintes colunas estão ausentes no arquivo: {missing_cols}\")\n",
    "else:\n",
    "    # Ordenar o DataFrame pelas colunas especificadas\n",
    "    df_sorted = df.sort_values(by=id_cols)\n",
    "\n",
    "    # Salvar o DataFrame ordenado em um novo arquivo CSV\n",
    "    df_sorted.to_csv('prio_01_sorted.csv', index=False)\n",
    "\n",
    "    print(\"Arquivo 'prio_01_sorted.csv' salvo com sucesso.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\1163526237.py:15: DtypeWarning: Columns (4,101,739,1150,1151,1178,1179,1180,1181,1183,1931,1957,1968,1971,1984,1989,1990,2042,2043,2061,2074,2076,2110,2126,2175,2183) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('Quali_20240602.csv', usecols=cols_to_read)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_02_sorted.csv' salvo com sucesso.\n"
     ]
    }
   ],
   "source": [
    "with open(\"colunas_especificas.txt\", 'r') as fin:\n",
    "    cols = fin.readlines()\n",
    "    cols_especificas = []\n",
    "    for c in cols:\n",
    "        cols_especificas.append(c.replace(\"\\n\",\"\"))\n",
    "\n",
    "\n",
    "# Defina as colunas para ordenação\n",
    "id_cols = ['ISIN', 'Citi Code', 'EPT Reference Language']\n",
    "\n",
    "# Combine as colunas necessárias para leitura (evita ler colunas desnecessárias)\n",
    "cols_to_read = cols_especificas + [col for col in id_cols if col not in cols_especificas]\n",
    "\n",
    "# Ler o arquivo 'Quali_20240602.csv' lendo apenas as colunas necessárias\n",
    "df = pd.read_csv('Quali_20240602.csv', usecols=cols_to_read)\n",
    "\n",
    "# Ordenar o DataFrame pelas colunas especificadas\n",
    "df_sorted = df.sort_values(by=id_cols)\n",
    "\n",
    "# Salvar o DataFrame filtrado e ordenado em um novo arquivo CSV\n",
    "df_sorted.to_csv('prio_02_sorted.csv', index=False)\n",
    "\n",
    "print(\"Arquivo 'prio_02_sorted.csv' salvo com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\3905341141.py:21: DtypeWarning: Columns (4,101,739,1150,1151,1178,1179,1180,1181,1183,1931,1957,1968,1971,1984,1990,2042,2043,2061,2074,2076,2110,2126,2175,2183,2203) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_01_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\3905341141.py:21: DtypeWarning: Columns (4,101,739,1150,1151,1178,1179,1180,1181,1183,1931,1957,1968,1971,1984,1989,1990,2042,2043,2061,2074,2076,2110,2126,2175,2183) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_02_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m output_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprio_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxx_formatted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_sorted.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Ler o arquivo 'Quali_202406xx.csv' lendo apenas as colunas necessárias\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcols_to_read\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Verificar se as colunas de ordenação estão presentes\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     missing_cols \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m id_cols \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns]\n",
      "File \u001b[1;32mc:\\Users\\Leo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Leo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Leo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Leo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen codecs>:331\u001b[0m, in \u001b[0;36mgetstate\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ler a lista de colunas específicas do arquivo 'colunas_especificas.txt'\n",
    "with open(\"colunas_especificas.txt\", 'r') as fin:\n",
    "    cols = fin.readlines()\n",
    "    cols_especificas = [c.strip() for c in cols]\n",
    "\n",
    "# Defina as colunas para ordenação\n",
    "id_cols = ['ISIN', 'Citi Code', 'EPT Reference Language']\n",
    "\n",
    "# Combine as colunas necessárias para leitura (evita ler colunas desnecessárias)\n",
    "cols_to_read = cols_especificas + [col for col in id_cols if col not in cols_especificas]\n",
    "\n",
    "for xx in range(1, 31):\n",
    "    xx_formatted = f\"{xx:02d}\"\n",
    "    input_filename = f\"Quali_202406{xx_formatted}.csv\"\n",
    "    output_filename = f\"prio_{xx_formatted}_sorted.csv\"\n",
    "\n",
    "    try:\n",
    "        # Ler o arquivo 'Quali_202406xx.csv' lendo apenas as colunas necessárias\n",
    "        df = pd.read_csv(input_filename, usecols=cols_to_read)\n",
    "\n",
    "        # Verificar se as colunas de ordenação estão presentes\n",
    "        missing_cols = [col for col in id_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"As seguintes colunas para ordenação estão ausentes no arquivo {input_filename}: {missing_cols}\")\n",
    "            continue\n",
    "\n",
    "        # Ordenar o DataFrame pelas colunas especificadas\n",
    "        df_sorted = df.sort_values(by=id_cols)\n",
    "\n",
    "        # Salvar o DataFrame filtrado e ordenado em um novo arquivo CSV\n",
    "        df_sorted.to_csv(output_filename, index=False)\n",
    "\n",
    "        print(f\"Arquivo '{output_filename}' salvo com sucesso.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"O arquivo '{input_filename}' não foi encontrado. Pulando para o próximo arquivo.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro ao processar o arquivo '{input_filename}': {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_01_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_02_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_03_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_04_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_05_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_06_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_07_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_08_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_09_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_10_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_11_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_12_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_13_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_14_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_15_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101,1150) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_16_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101,1150) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_17_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101,1150) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_18_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101,1150) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_19_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101,1150) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_20_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101,1150) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_21_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101,1150) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_22_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101,1150) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_23_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101,1150) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_24_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101,1150) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_25_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101,1150) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_26_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101,1150) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_27_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101,1150) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_28_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101,1150) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_29_sorted.csv' salvo com sucesso.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Leo\\AppData\\Local\\Temp\\ipykernel_5716\\643419753.py:26: DtypeWarning: Columns (101,1150) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'prio_30_sorted.csv' salvo com sucesso.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ler a lista de colunas específicas do arquivo 'colunas_especificas.txt'\n",
    "with open(\"colunas_especificas.txt\", 'r') as fin:\n",
    "    cols = fin.readlines()\n",
    "    cols_especificas = [c.strip() for c in cols]\n",
    "\n",
    "# Defina as colunas para ordenação\n",
    "id_cols = ['ISIN', 'Citi Code', 'EPT Reference Language']\n",
    "\n",
    "# Combine as colunas necessárias para leitura (evita ler colunas desnecessárias)\n",
    "cols_to_read = cols_especificas + [col for col in id_cols if col not in cols_especificas]\n",
    "\n",
    "# Definir o tamanho do chunk (número de linhas a serem lidas)\n",
    "# Se chunk_size for None, o script lerá o arquivo inteiro\n",
    "chunk_size = 1000  # Defina para um número inteiro, por exemplo, 1000, para ler apenas 1000 linhas\n",
    "\n",
    "for xx in range(1, 31):\n",
    "    xx_formatted = f\"{xx:02d}\"\n",
    "    input_filename = f\"Quali_202406{xx_formatted}.csv\"\n",
    "    output_filename = f\"prio_{xx_formatted}_sorted.csv\"\n",
    "\n",
    "    try:\n",
    "        if chunk_size is not None:\n",
    "            # Ler apenas as primeiras 'chunk_size' linhas\n",
    "            df = pd.read_csv(input_filename, usecols=cols_to_read, nrows=chunk_size)\n",
    "        else:\n",
    "            # Ler o arquivo inteiro\n",
    "            df = pd.read_csv(input_filename, usecols=cols_to_read)\n",
    "\n",
    "        # Verificar se as colunas de ordenação estão presentes\n",
    "        missing_cols = [col for col in id_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"As seguintes colunas para ordenação estão ausentes no arquivo {input_filename}: {missing_cols}\")\n",
    "            continue\n",
    "\n",
    "        # Ordenar o DataFrame pelas colunas especificadas\n",
    "        df_sorted = df.sort_values(by=id_cols)\n",
    "\n",
    "        # Salvar o DataFrame filtrado e ordenado em um novo arquivo CSV\n",
    "        df_sorted.to_csv(output_filename, index=False)\n",
    "\n",
    "        print(f\"Arquivo '{output_filename}' salvo com sucesso.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"O arquivo '{input_filename}' não foi encontrado. Pulando para o próximo arquivo.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro ao processar o arquivo '{input_filename}': {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando chunk 30 de 30\n",
      "Todos os chunks foram processados.\n",
      "              ISIN Citi Code EPT Reference Language  \\\n",
      "0     AT0000607254      03X1                    deu   \n",
      "201   AT0000636964      03X5                    deu   \n",
      "5179  LU0607983383      015B                    deu   \n",
      "291   AT0000642780      03WW                    deu   \n",
      "5118  LU0605964849      03Z1                    eng   \n",
      "...            ...       ...                    ...   \n",
      "585   AT0000711593      041L                    deu   \n",
      "584   AT0000711593      041L                    deu   \n",
      "960   AT0000859236      03SR                    deu   \n",
      "2172  IE00B61TKZ25      00AL                    dan   \n",
      "4698  LU0562087980      03ZG                    eng   \n",
      "\n",
      "                                            campo  valor_antigo    valor_novo  \\\n",
      "0     20000_Financial_Instrument_Identifying_Data  AT0000607254           NaN   \n",
      "201   20000_Financial_Instrument_Identifying_Data  AT0000636964           NaN   \n",
      "5179  20000_Financial_Instrument_Identifying_Data  LU0607983383           NaN   \n",
      "291   20000_Financial_Instrument_Identifying_Data  AT0000642780           NaN   \n",
      "5118  20000_Financial_Instrument_Identifying_Data  LU0605964849           NaN   \n",
      "...                                           ...           ...           ...   \n",
      "585   20000_Financial_Instrument_Identifying_Data  AT0000711593           NaN   \n",
      "584   20000_Financial_Instrument_Identifying_Data           NaN  AT0000711593   \n",
      "960   20000_Financial_Instrument_Identifying_Data  AT0000859236           NaN   \n",
      "2172  20000_Financial_Instrument_Identifying_Data           NaN  IE00B61TKZ25   \n",
      "4698  20000_Financial_Instrument_Identifying_Data           NaN  LU0562087980   \n",
      "\n",
      "            data  \n",
      "0     01/06/2024  \n",
      "201   01/06/2024  \n",
      "5179  01/06/2024  \n",
      "291   01/06/2024  \n",
      "5118  01/06/2024  \n",
      "...          ...  \n",
      "585   30/06/2024  \n",
      "584   30/06/2024  \n",
      "960   30/06/2024  \n",
      "2172  30/06/2024  \n",
      "4698  30/06/2024  \n",
      "\n",
      "[5911 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Defina as colunas identificadoras\n",
    "id_cols = ['ISIN', 'Citi Code', 'EPT Reference Language']\n",
    "\n",
    "# Lista para armazenar os DataFrames com a coluna 'data' adicionada\n",
    "dfs_with_date = []\n",
    "\n",
    "total_chunks = 30  # Total de chunks a serem processados\n",
    "\n",
    "for xx in range(1, total_chunks + 1):\n",
    "    xx_formatted = f\"{xx:02d}\"\n",
    "    filename = f\"prio_{xx_formatted}_sorted.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Ler o arquivo 'prio_xx_sorted.csv'\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "        # Adiciona a coluna 'data' com o valor do dia no formato 'xx/06/2024'\n",
    "        df['data'] = f\"{xx_formatted}/06/2024\"\n",
    "        \n",
    "        # Certifique-se de que as colunas identificadoras estão presentes\n",
    "        missing_cols = [col for col in id_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"\\nAs seguintes colunas estão ausentes no arquivo {filename}: {missing_cols}\")\n",
    "            continue\n",
    "        \n",
    "        dfs_with_date.append(df)\n",
    "        \n",
    "        # Imprimir o progresso e apagar a linha anterior\n",
    "        sys.stdout.write(f\"\\rProcessando chunk {xx} de {total_chunks}\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nO arquivo '{filename}' não foi encontrado. Pulando para o próximo arquivo.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"\\nOcorreu um erro ao processar o arquivo '{filename}': {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\nTodos os chunks foram processados.\")\n",
    "\n",
    "# Combina todos os DataFrames em um único DataFrame\n",
    "combined_df = pd.concat(dfs_with_date, ignore_index=True)\n",
    "\n",
    "# Converter a coluna 'data' para datetime para facilitar a ordenação\n",
    "combined_df['data'] = pd.to_datetime(combined_df['data'], format='%d/%m/%Y')\n",
    "\n",
    "# Ordena o DataFrame pelos identificadores e pela data\n",
    "combined_df.sort_values(by=id_cols + ['data'], inplace=True)\n",
    "\n",
    "# Lista para armazenar as mudanças detectadas\n",
    "changes = []\n",
    "\n",
    "# Definir as colunas a serem comparadas (todas exceto as identificadoras e 'data')\n",
    "value_cols = [col for col in combined_df.columns if col not in id_cols + ['data']]\n",
    "\n",
    "# Agrupa o DataFrame pelos identificadores\n",
    "grouped = combined_df.groupby(id_cols)\n",
    "\n",
    "for name, group in grouped:\n",
    "    group = group.sort_values(by='data').reset_index(drop=True)\n",
    "    for i in range(1, len(group)):\n",
    "        current_row = group.loc[i]\n",
    "        previous_row = group.loc[i - 1]\n",
    "        data_current = current_row['data'].strftime('%d/%m/%Y')  # Converte para string no formato desejado\n",
    "        data_previous = previous_row['data'].strftime('%d/%m/%Y')\n",
    "        for col in value_cols:\n",
    "            value_current = current_row[col]\n",
    "            value_previous = previous_row[col]\n",
    "            # Verifica se os valores são diferentes, considerando valores nulos\n",
    "            if pd.isnull(value_current) and pd.isnull(value_previous):\n",
    "                continue  # Ambos são nulos, não há mudança\n",
    "            elif pd.isnull(value_current) != pd.isnull(value_previous):\n",
    "                # Um valor é nulo e o outro não\n",
    "                changes.append({\n",
    "                    'ISIN': current_row['ISIN'],\n",
    "                    'Citi Code': current_row['Citi Code'],\n",
    "                    'EPT Reference Language': current_row['EPT Reference Language'],\n",
    "                    'campo': col,\n",
    "                    'valor_antigo': value_previous,\n",
    "                    'valor_novo': value_current,\n",
    "                    'data': data_current\n",
    "                })\n",
    "            elif value_current != value_previous:\n",
    "                changes.append({\n",
    "                    'ISIN': current_row['ISIN'],\n",
    "                    'Citi Code': current_row['Citi Code'],\n",
    "                    'EPT Reference Language': current_row['EPT Reference Language'],\n",
    "                    'campo': col,\n",
    "                    'valor_antigo': value_previous,\n",
    "                    'valor_novo': value_current,\n",
    "                    'data': data_current\n",
    "                })\n",
    "\n",
    "# Cria um DataFrame com as mudanças\n",
    "changes_df = pd.DataFrame(changes)\n",
    "\n",
    "# Converter a coluna 'data' para datetime para ordenação\n",
    "changes_df['data'] = pd.to_datetime(changes_df['data'], format='%d/%m/%Y')\n",
    "\n",
    "# Ordena o DataFrame 'changes_df' pela coluna 'data'\n",
    "changes_df.sort_values(by='data', inplace=True)\n",
    "\n",
    "# Opcional: Converte a coluna 'data' de volta para string no formato desejado\n",
    "changes_df['data'] = changes_df['data'].dt.strftime('%d/%m/%Y')\n",
    "\n",
    "# Seleciona apenas as colunas desejadas\n",
    "changes_df = changes_df[['ISIN', 'Citi Code', 'EPT Reference Language', 'campo', 'valor_antigo', 'valor_novo', 'data']]\n",
    "\n",
    "# Exibe o DataFrame com as mudanças ordenadas por data\n",
    "print(changes_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISIN</th>\n",
       "      <th>Citi Code</th>\n",
       "      <th>EPT Reference Language</th>\n",
       "      <th>campo</th>\n",
       "      <th>valor_antigo</th>\n",
       "      <th>valor_novo</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AT0000607254</td>\n",
       "      <td>03X1</td>\n",
       "      <td>deu</td>\n",
       "      <td>20000_Financial_Instrument_Identifying_Data</td>\n",
       "      <td>AT0000607254</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01/06/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>AT0000636964</td>\n",
       "      <td>03X5</td>\n",
       "      <td>deu</td>\n",
       "      <td>20000_Financial_Instrument_Identifying_Data</td>\n",
       "      <td>AT0000636964</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01/06/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5179</th>\n",
       "      <td>LU0607983383</td>\n",
       "      <td>015B</td>\n",
       "      <td>deu</td>\n",
       "      <td>20000_Financial_Instrument_Identifying_Data</td>\n",
       "      <td>LU0607983383</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01/06/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>AT0000642780</td>\n",
       "      <td>03WW</td>\n",
       "      <td>deu</td>\n",
       "      <td>20000_Financial_Instrument_Identifying_Data</td>\n",
       "      <td>AT0000642780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01/06/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5118</th>\n",
       "      <td>LU0605964849</td>\n",
       "      <td>03Z1</td>\n",
       "      <td>eng</td>\n",
       "      <td>20000_Financial_Instrument_Identifying_Data</td>\n",
       "      <td>LU0605964849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01/06/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>AT0000711593</td>\n",
       "      <td>041L</td>\n",
       "      <td>deu</td>\n",
       "      <td>20000_Financial_Instrument_Identifying_Data</td>\n",
       "      <td>AT0000711593</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30/06/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>AT0000711593</td>\n",
       "      <td>041L</td>\n",
       "      <td>deu</td>\n",
       "      <td>20000_Financial_Instrument_Identifying_Data</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AT0000711593</td>\n",
       "      <td>30/06/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>AT0000859236</td>\n",
       "      <td>03SR</td>\n",
       "      <td>deu</td>\n",
       "      <td>20000_Financial_Instrument_Identifying_Data</td>\n",
       "      <td>AT0000859236</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30/06/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2172</th>\n",
       "      <td>IE00B61TKZ25</td>\n",
       "      <td>00AL</td>\n",
       "      <td>dan</td>\n",
       "      <td>20000_Financial_Instrument_Identifying_Data</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IE00B61TKZ25</td>\n",
       "      <td>30/06/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4698</th>\n",
       "      <td>LU0562087980</td>\n",
       "      <td>03ZG</td>\n",
       "      <td>eng</td>\n",
       "      <td>20000_Financial_Instrument_Identifying_Data</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LU0562087980</td>\n",
       "      <td>30/06/2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5911 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ISIN Citi Code EPT Reference Language  \\\n",
       "0     AT0000607254      03X1                    deu   \n",
       "201   AT0000636964      03X5                    deu   \n",
       "5179  LU0607983383      015B                    deu   \n",
       "291   AT0000642780      03WW                    deu   \n",
       "5118  LU0605964849      03Z1                    eng   \n",
       "...            ...       ...                    ...   \n",
       "585   AT0000711593      041L                    deu   \n",
       "584   AT0000711593      041L                    deu   \n",
       "960   AT0000859236      03SR                    deu   \n",
       "2172  IE00B61TKZ25      00AL                    dan   \n",
       "4698  LU0562087980      03ZG                    eng   \n",
       "\n",
       "                                            campo  valor_antigo    valor_novo  \\\n",
       "0     20000_Financial_Instrument_Identifying_Data  AT0000607254           NaN   \n",
       "201   20000_Financial_Instrument_Identifying_Data  AT0000636964           NaN   \n",
       "5179  20000_Financial_Instrument_Identifying_Data  LU0607983383           NaN   \n",
       "291   20000_Financial_Instrument_Identifying_Data  AT0000642780           NaN   \n",
       "5118  20000_Financial_Instrument_Identifying_Data  LU0605964849           NaN   \n",
       "...                                           ...           ...           ...   \n",
       "585   20000_Financial_Instrument_Identifying_Data  AT0000711593           NaN   \n",
       "584   20000_Financial_Instrument_Identifying_Data           NaN  AT0000711593   \n",
       "960   20000_Financial_Instrument_Identifying_Data  AT0000859236           NaN   \n",
       "2172  20000_Financial_Instrument_Identifying_Data           NaN  IE00B61TKZ25   \n",
       "4698  20000_Financial_Instrument_Identifying_Data           NaN  LU0562087980   \n",
       "\n",
       "            data  \n",
       "0     01/06/2024  \n",
       "201   01/06/2024  \n",
       "5179  01/06/2024  \n",
       "291   01/06/2024  \n",
       "5118  01/06/2024  \n",
       "...          ...  \n",
       "585   30/06/2024  \n",
       "584   30/06/2024  \n",
       "960   30/06/2024  \n",
       "2172  30/06/2024  \n",
       "4698  30/06/2024  \n",
       "\n",
       "[5911 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando chunk 30 de 30\n",
      "Todos os chunks foram processados.\n",
      "              ISIN Citi Code EPT Reference Language  \\\n",
      "0     AT0000607254      03X1                    deu   \n",
      "201   AT0000636964      03X5                    deu   \n",
      "5179  LU0607983383      015B                    deu   \n",
      "291   AT0000642780      03WW                    deu   \n",
      "5118  LU0605964849      03Z1                    eng   \n",
      "...            ...       ...                    ...   \n",
      "585   AT0000711593      041L                    deu   \n",
      "584   AT0000711593      041L                    deu   \n",
      "960   AT0000859236      03SR                    deu   \n",
      "2172  IE00B61TKZ25      00AL                    dan   \n",
      "4698  LU0562087980      03ZG                    eng   \n",
      "\n",
      "                                            campo  valor_antigo    valor_novo  \\\n",
      "0     20000_Financial_Instrument_Identifying_Data  AT0000607254           NaN   \n",
      "201   20000_Financial_Instrument_Identifying_Data  AT0000636964           NaN   \n",
      "5179  20000_Financial_Instrument_Identifying_Data  LU0607983383           NaN   \n",
      "291   20000_Financial_Instrument_Identifying_Data  AT0000642780           NaN   \n",
      "5118  20000_Financial_Instrument_Identifying_Data  LU0605964849           NaN   \n",
      "...                                           ...           ...           ...   \n",
      "585   20000_Financial_Instrument_Identifying_Data  AT0000711593           NaN   \n",
      "584   20000_Financial_Instrument_Identifying_Data           NaN  AT0000711593   \n",
      "960   20000_Financial_Instrument_Identifying_Data  AT0000859236           NaN   \n",
      "2172  20000_Financial_Instrument_Identifying_Data           NaN  IE00B61TKZ25   \n",
      "4698  20000_Financial_Instrument_Identifying_Data           NaN  LU0562087980   \n",
      "\n",
      "            data  \n",
      "0     01/06/2024  \n",
      "201   01/06/2024  \n",
      "5179  01/06/2024  \n",
      "291   01/06/2024  \n",
      "5118  01/06/2024  \n",
      "...          ...  \n",
      "585   30/06/2024  \n",
      "584   30/06/2024  \n",
      "960   30/06/2024  \n",
      "2172  30/06/2024  \n",
      "4698  30/06/2024  \n",
      "\n",
      "[5911 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Defina as colunas identificadoras\n",
    "id_cols = ['ISIN', 'Citi Code', 'EPT Reference Language']\n",
    "\n",
    "# Lista para armazenar os DataFrames com a coluna 'data' adicionada\n",
    "dfs_with_date = []\n",
    "\n",
    "total_chunks = 30  # Total de chunks a serem processados\n",
    "\n",
    "for xx in range(1, total_chunks + 1):\n",
    "    xx_formatted = f\"{xx:02d}\"\n",
    "    filename = f\"prio_{xx_formatted}_sorted.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Ler o arquivo 'prio_xx_sorted.csv'\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "        # Adiciona a coluna 'data' com o valor do dia no formato 'xx/06/2024'\n",
    "        df['data'] = f\"{xx_formatted}/06/2024\"\n",
    "        \n",
    "        # Certifique-se de que as colunas identificadoras estão presentes\n",
    "        missing_cols = [col for col in id_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"\\nAs seguintes colunas estão ausentes no arquivo {filename}: {missing_cols}\")\n",
    "            continue\n",
    "        \n",
    "        dfs_with_date.append(df)\n",
    "        \n",
    "        # Imprimir o progresso e apagar a linha anterior\n",
    "        sys.stdout.write(f\"\\rProcessando chunk {xx} de {total_chunks}\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nO arquivo '{filename}' não foi encontrado. Pulando para o próximo arquivo.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"\\nOcorreu um erro ao processar o arquivo '{filename}': {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\nTodos os chunks foram processados.\")\n",
    "\n",
    "# Combina todos os DataFrames em um único DataFrame\n",
    "combined_df = pd.concat(dfs_with_date, ignore_index=True)\n",
    "\n",
    "# Converter a coluna 'data' para datetime para facilitar a ordenação\n",
    "combined_df['data'] = pd.to_datetime(combined_df['data'], format='%d/%m/%Y')\n",
    "\n",
    "# Ordena o DataFrame pelos identificadores e pela data\n",
    "combined_df.sort_values(by=id_cols + ['data'], inplace=True)\n",
    "\n",
    "# Lista para armazenar as mudanças detectadas\n",
    "changes = []\n",
    "\n",
    "# Definir as colunas a serem comparadas (todas exceto as identificadoras e 'data')\n",
    "value_cols = [col for col in combined_df.columns if col not in id_cols + ['data']]\n",
    "\n",
    "# Agrupa o DataFrame pelos identificadores\n",
    "grouped = combined_df.groupby(id_cols)\n",
    "\n",
    "for name, group in grouped:\n",
    "    group = group.sort_values(by='data').reset_index(drop=True)\n",
    "    for i in range(1, len(group)):\n",
    "        current_row = group.loc[i]\n",
    "        previous_row = group.loc[i - 1]\n",
    "        data_current = current_row['data'].strftime('%d/%m/%Y')  # Data atual\n",
    "        data_previous = previous_row['data'].strftime('%d/%m/%Y')  # Data anterior\n",
    "        for col in value_cols:\n",
    "            value_current = current_row[col]\n",
    "            value_previous = previous_row[col]\n",
    "            # Ignorar comparação se o valor anterior é nulo e é o primeiro registro\n",
    "            if i == 1 and pd.isnull(value_previous):\n",
    "                continue\n",
    "            # Verifica se os valores são diferentes, considerando valores nulos\n",
    "            if pd.isnull(value_current) and pd.isnull(value_previous):\n",
    "                continue  # Ambos são nulos, não há mudança\n",
    "            elif pd.isnull(value_current) != pd.isnull(value_previous):\n",
    "                # Um valor é nulo e o outro não\n",
    "                changes.append({\n",
    "                    'ISIN': current_row['ISIN'],\n",
    "                    'Citi Code': current_row['Citi Code'],\n",
    "                    'EPT Reference Language': current_row['EPT Reference Language'],\n",
    "                    'campo': col,\n",
    "                    'valor_antigo': value_previous,\n",
    "                    'valor_novo': value_current,\n",
    "                    'data': data_current\n",
    "                })\n",
    "            elif value_current != value_previous:\n",
    "                changes.append({\n",
    "                    'ISIN': current_row['ISIN'],\n",
    "                    'Citi Code': current_row['Citi Code'],\n",
    "                    'EPT Reference Language': current_row['EPT Reference Language'],\n",
    "                    'campo': col,\n",
    "                    'valor_antigo': value_previous,\n",
    "                    'valor_novo': value_current,\n",
    "                    'data': data_current\n",
    "                })\n",
    "\n",
    "# Cria um DataFrame com as mudanças\n",
    "changes_df = pd.DataFrame(changes)\n",
    "\n",
    "# Verifica se o DataFrame de mudanças não está vazio\n",
    "if not changes_df.empty:\n",
    "    # Converter a coluna 'data' para datetime para ordenação\n",
    "    changes_df['data'] = pd.to_datetime(changes_df['data'], format='%d/%m/%Y')\n",
    "\n",
    "    # Ordena o DataFrame 'changes_df' pela coluna 'data'\n",
    "    changes_df.sort_values(by='data', inplace=True)\n",
    "\n",
    "    # Opcional: Converte a coluna 'data' de volta para string no formato desejado\n",
    "    changes_df['data'] = changes_df['data'].dt.strftime('%d/%m/%Y')\n",
    "\n",
    "    # Seleciona apenas as colunas desejadas\n",
    "    changes_df = changes_df[['ISIN', 'Citi Code', 'EPT Reference Language', 'campo', 'valor_antigo', 'valor_novo', 'data']]\n",
    "\n",
    "    # Exibe o DataFrame com as mudanças ordenadas por data\n",
    "    print(changes_df)\n",
    "else:\n",
    "    print(\"Nenhuma mudança foi detectada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processando chunk 30 de 30\n",
      "Todos os chunks foram processados.\n",
      "              ISIN Citi Code EPT Reference Language  \\\n",
      "0     AT0000607254      03X1                    deu   \n",
      "201   AT0000636964      03X5                    deu   \n",
      "5179  LU0607983383      015B                    deu   \n",
      "291   AT0000642780      03WW                    deu   \n",
      "5118  LU0605964849      03Z1                    eng   \n",
      "...            ...       ...                    ...   \n",
      "585   AT0000711593      041L                    deu   \n",
      "584   AT0000711593      041L                    deu   \n",
      "960   AT0000859236      03SR                    deu   \n",
      "2172  IE00B61TKZ25      00AL                    dan   \n",
      "4698  LU0562087980      03ZG                    eng   \n",
      "\n",
      "                                            campo  valor_antigo    valor_novo  \\\n",
      "0     20000_Financial_Instrument_Identifying_Data  AT0000607254           NaN   \n",
      "201   20000_Financial_Instrument_Identifying_Data  AT0000636964           NaN   \n",
      "5179  20000_Financial_Instrument_Identifying_Data  LU0607983383           NaN   \n",
      "291   20000_Financial_Instrument_Identifying_Data  AT0000642780           NaN   \n",
      "5118  20000_Financial_Instrument_Identifying_Data  LU0605964849           NaN   \n",
      "...                                           ...           ...           ...   \n",
      "585   20000_Financial_Instrument_Identifying_Data  AT0000711593           NaN   \n",
      "584   20000_Financial_Instrument_Identifying_Data           NaN  AT0000711593   \n",
      "960   20000_Financial_Instrument_Identifying_Data  AT0000859236           NaN   \n",
      "2172  20000_Financial_Instrument_Identifying_Data           NaN  IE00B61TKZ25   \n",
      "4698  20000_Financial_Instrument_Identifying_Data           NaN  LU0562087980   \n",
      "\n",
      "            data  \n",
      "0     01/06/2024  \n",
      "201   01/06/2024  \n",
      "5179  01/06/2024  \n",
      "291   01/06/2024  \n",
      "5118  01/06/2024  \n",
      "...          ...  \n",
      "585   30/06/2024  \n",
      "584   30/06/2024  \n",
      "960   30/06/2024  \n",
      "2172  30/06/2024  \n",
      "4698  30/06/2024  \n",
      "\n",
      "[5911 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Defina as colunas identificadoras\n",
    "id_cols = ['ISIN', 'Citi Code', 'EPT Reference Language']\n",
    "\n",
    "# Lista para armazenar os DataFrames com a coluna 'data' adicionada\n",
    "dfs_with_date = []\n",
    "\n",
    "total_chunks = 30  # Total de chunks a serem processados\n",
    "\n",
    "for xx in range(1, total_chunks + 1):\n",
    "    xx_formatted = f\"{xx:02d}\"\n",
    "    filename = f\"prio_{xx_formatted}_sorted.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Ler o arquivo 'prio_xx_sorted.csv'\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "        # Adiciona a coluna 'data' com o valor do dia no formato 'xx/06/2024'\n",
    "        df['data'] = f\"{xx_formatted}/06/2024\"\n",
    "        \n",
    "        # Certifique-se de que as colunas identificadoras estão presentes\n",
    "        missing_cols = [col for col in id_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"\\nAs seguintes colunas estão ausentes no arquivo {filename}: {missing_cols}\")\n",
    "            continue\n",
    "        \n",
    "        dfs_with_date.append(df)\n",
    "        \n",
    "        # Imprimir o progresso e apagar a linha anterior\n",
    "        sys.stdout.write(f\"\\rProcessando chunk {xx} de {total_chunks}\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nO arquivo '{filename}' não foi encontrado. Pulando para o próximo arquivo.\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"\\nOcorreu um erro ao processar o arquivo '{filename}': {e}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\nTodos os chunks foram processados.\")\n",
    "\n",
    "# Combina todos os DataFrames em um único DataFrame\n",
    "combined_df = pd.concat(dfs_with_date, ignore_index=True)\n",
    "\n",
    "# Converter a coluna 'data' para datetime para facilitar a ordenação\n",
    "combined_df['data'] = pd.to_datetime(combined_df['data'], format='%d/%m/%Y')\n",
    "\n",
    "# Ordena o DataFrame pelos identificadores e pela data\n",
    "combined_df.sort_values(by=id_cols + ['data'], inplace=True)\n",
    "\n",
    "# Lista para armazenar as mudanças detectadas\n",
    "changes = []\n",
    "\n",
    "# Definir as colunas a serem comparadas (todas exceto as identificadoras e 'data')\n",
    "value_cols = [col for col in combined_df.columns if col not in id_cols + ['data']]\n",
    "\n",
    "# Agrupa o DataFrame pelos identificadores\n",
    "grouped = combined_df.groupby(id_cols)\n",
    "\n",
    "for name, group in grouped:\n",
    "    group = group.sort_values(by='data').reset_index(drop=True)\n",
    "    for i in range(1, len(group)):\n",
    "        current_row = group.loc[i]\n",
    "        previous_row = group.loc[i - 1]\n",
    "        data_current = current_row['data'].strftime('%d/%m/%Y')  # Data atual\n",
    "        data_previous = previous_row['data'].strftime('%d/%m/%Y')  # Data anterior\n",
    "        for col in value_cols:\n",
    "            value_current = current_row[col]\n",
    "            value_previous = previous_row[col]\n",
    "            # Ignorar comparação se o valor anterior é nulo e é o primeiro registro\n",
    "            if i == 1 and pd.isnull(value_previous):\n",
    "                continue\n",
    "            # Verifica se os valores são diferentes, considerando valores nulos\n",
    "            if pd.isnull(value_current) and pd.isnull(value_previous):\n",
    "                continue  # Ambos são nulos, não há mudança\n",
    "            elif pd.isnull(value_current) != pd.isnull(value_previous):\n",
    "                # Um valor é nulo e o outro não\n",
    "                changes.append({\n",
    "                    'ISIN': current_row['ISIN'],\n",
    "                    'Citi Code': current_row['Citi Code'],\n",
    "                    'EPT Reference Language': current_row['EPT Reference Language'],\n",
    "                    'campo': col,\n",
    "                    'valor_antigo': value_previous,\n",
    "                    'valor_novo': value_current,\n",
    "                    'data': data_current\n",
    "                })\n",
    "            elif value_current != value_previous:\n",
    "                changes.append({\n",
    "                    'ISIN': current_row['ISIN'],\n",
    "                    'Citi Code': current_row['Citi Code'],\n",
    "                    'EPT Reference Language': current_row['EPT Reference Language'],\n",
    "                    'campo': col,\n",
    "                    'valor_antigo': value_previous,\n",
    "                    'valor_novo': value_current,\n",
    "                    'data': data_current\n",
    "                })\n",
    "\n",
    "# Cria um DataFrame com as mudanças\n",
    "changes_df = pd.DataFrame(changes)\n",
    "\n",
    "# Verifica se o DataFrame de mudanças não está vazio\n",
    "if not changes_df.empty:\n",
    "    # Converter a coluna 'data' para datetime para ordenação\n",
    "    changes_df['data'] = pd.to_datetime(changes_df['data'], format='%d/%m/%Y')\n",
    "\n",
    "    # Ordena o DataFrame 'changes_df' pela coluna 'data'\n",
    "    changes_df.sort_values(by='data', inplace=True)\n",
    "\n",
    "    # Opcional: Converte a coluna 'data' de volta para string no formato desejado\n",
    "    changes_df['data'] = changes_df['data'].dt.strftime('%d/%m/%Y')\n",
    "\n",
    "    # Seleciona apenas as colunas desejadas\n",
    "    changes_df = changes_df[['ISIN', 'Citi Code', 'EPT Reference Language', 'campo', 'valor_antigo', 'valor_novo', 'data']]\n",
    "\n",
    "    # Exibe o DataFrame com as mudanças ordenadas por data\n",
    "    print(changes_df)\n",
    "else:\n",
    "    print(\"Nenhuma mudança foi detectada.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
